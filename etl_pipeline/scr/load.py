import pandas as pd
import logging
from sqlalchemy import text
from pathlib import Path
from config import get_engine, check_connection

logger = logging.getLogger(__name__)

BASE_DIR = Path(__file__).resolve().parent.parent
PROCESSED_DIR = BASE_DIR / 'data' / 'processed'
INPUT_FILE = PROCESSED_DIR / 'renewable_energy_data_final.csv'


def load_dimensions(df, conn):

    logger.info("Carregando dimens√µes...")

    # dim_country
    logger.info("  ‚Üí dim_country...")
    df_country = df[['country', 'iso3_code', 'm49_code', 'region', 'sub_region']].drop_duplicates()
    df_country.to_sql('dim_country', conn, if_exists='append', index=False, method='multi')
    logger.info(f"    ‚úÖ {len(df_country)} pa√≠ses inseridos")

    # dim_technology
    logger.info("  ‚Üí dim_technology...")
    df_tech = df[['technology', 'sub_technology', 'group_technology', 'renewable_or_not']].drop_duplicates()
    df_tech.to_sql('dim_technology', conn, if_exists='append', index=False, method='multi')
    logger.info(f"    ‚úÖ {len(df_tech)} tecnologias inseridas")

    # dim_time
    logger.info("  ‚Üí dim_time...")
    df_time = df[['year']].drop_duplicates()
    df_time['decade'] = (df_time['year'] // 10) * 10
    df_time.to_sql('dim_time', conn, if_exists='append', index=False, method='multi')
    logger.info(f"    ‚úÖ {len(df_time)} anos inseridos")

    # dim_producer
    logger.info("  ‚Üí dim_producer...")
    df_producer = df[['producer_type']].drop_duplicates()
    df_producer.to_sql('dim_producer', conn, if_exists='append', index=False, method='multi')
    logger.info(f"    ‚úÖ {len(df_producer)} tipos de produtor inseridos")

    logger.info("‚úÖ Dimens√µes carregadas!\n")


def get_ids_dimensions(df, conn):

    logger.info("Mapeando IDs das dimens√µes...")

    # L√™ as dimens√µes do banco
    dim_country = pd.read_sql('SELECT country_id, country FROM dim_country', conn)
    dim_tech = pd.read_sql('SELECT technology_id, technology FROM dim_technology', conn)
    dim_time = pd.read_sql('SELECT time_id, year FROM dim_time', conn)
    dim_producer = pd.read_sql('SELECT producer_id, producer_type FROM dim_producer', conn)
    
    # Faz merge para obter IDs
    df = df.merge(dim_country, on='country', how='left')
    df = df.merge(dim_tech, on='technology', how='left')
    df = df.merge(dim_time, on='year', how='left')
    df = df.merge(dim_producer, on='producer_type', how='left')
    
    logger.info("‚úÖ IDs mapeados!\n")
    
    return df


def load_fact(df, conn):

    logger.info("Carregando tabela fato...")

    # Seleciona apenas colunas necess√°rias
    df_fact = df[[
        'country_id', 'technology_id', 'time_id', 'producer_id',
        'electricity_generation_gwh', 'electricity_installed_capacity_mw', 'heat_generation_tj',
        'total_public_flows_usd_m', 'international_public_flows_usd_m',
        'capacity_per_capita_w'
    ]]

    # Insere
    df_fact.to_sql('fact_energy_generation', conn, if_exists='append', index=False, method='multi')
    
    logger.info(f"‚úÖ {len(df_fact)} registros inseridos na tabela fato!\n")


def load_data(df):
    """Pipeline completo de carga"""
    logger.info("="*60)
    logger.info("üì§ INICIANDO CARGA NO DATA WAREHOUSE")
    logger.info("="*60 + "\n")

    if not check_connection():
        return

    try:

        engine = get_engine()    

        with engine.begin() as conn:
            # Limpa tabelas antes de recarregar (evita duplicatas)
            logger.info("üóëÔ∏è  Limpando tabelas...")
            conn.execute(text("TRUNCATE TABLE fact_energy_generation, dim_country, dim_technology, dim_time, dim_producer RESTART IDENTITY CASCADE"))
            logger.info("‚úÖ Tabelas limpas!\n")

            # Carrega dimens√µes
            load_dimensions(df, conn)

            # Mapeia IDs
            df_with_ids = get_ids_dimensions(df, conn)

            # Carrega fato
            load_fact(df_with_ids, conn)
        
        logger.info("="*60)
        logger.info("‚úÖ CARGA CONCLU√çDA COM SUCESSO!")
        logger.info("="*60)
        
    except Exception as e:
        logger.error(f"‚ùå Erro na carga: {e}")
        raise


if __name__ == "__main__":
    handler = logging.StreamHandler()
    handler.setFormatter(logging.Formatter("%(message)s"))
    logger.addHandler(handler)
    logger.setLevel(logging.INFO)

    if INPUT_FILE.exists():
        df = pd.read_csv(INPUT_FILE)
        load_data(df)
    else:
        logger.error(f"‚ùå Arquivo n√£o encontrado: {INPUT_FILE}")    